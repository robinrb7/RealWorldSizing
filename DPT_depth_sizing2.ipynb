{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOL/+WjSu+/qyEfc3sUf6KV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robinrb7/RealWorldSizing/blob/main/DPT_depth_sizing2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFMG9HLZi4py",
        "outputId": "13271179-8b0e-4e68-cd2d-655304edfa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: exifread in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision) (1.3.1)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from supervision) (2.0.2)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from supervision) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.67.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install necessary libraries (run this in a Colab cell)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers\n",
        "!pip install pillow requests supervision exifread"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8vrJzbE9pkB",
        "outputId": "0204a4d0-22bd-4066-bbc3-25a9e721e736"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu118\n",
            "Uninstalling torch-2.6.0+cu118:\n",
            "  Successfully uninstalled torch-2.6.0+cu118\n",
            "Found existing installation: torchvision 0.21.0+cu118\n",
            "Uninstalling torchvision-0.21.0+cu118:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu118\n",
            "Found existing installation: torchaudio 2.6.0+cu118\n",
            "Uninstalling torchaudio-2.6.0+cu118:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch with CUDA 11.8\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU_YxyAC9sNi",
        "outputId": "f16968b9-961f-4140-bb94-45708608c563"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "import supervision as sv\n",
        "import exifread\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    AutoProcessor, AutoModelForZeroShotObjectDetection,\n",
        "    SamModel, SamProcessor, AutoImageProcessor, AutoModelForDepthEstimation\n",
        ")"
      ],
      "metadata": {
        "id": "9H5vWsc89uSf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load Models\n",
        "\n",
        "# Grounding DINO\n",
        "model_id = \"IDEA-Research/grounding-dino-base\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "detection_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjiARRhJ9wbz",
        "outputId": "7bca46be-cf18-464f-93ad-1fea260b1762"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment Anything Model (SAM)\n",
        "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
        "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "QgaD2Fkq9yge"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depth Estimation with DPT-Large (NYU Depth V2)\n",
        "depth_model_id = \"Intel/dpt-large\"  # Using DPT-Large fine-tuned on NYU Depth V2\n",
        "depth_processor = AutoImageProcessor.from_pretrained(depth_model_id)\n",
        "depth_model = AutoModelForDepthEstimation.from_pretrained(depth_model_id).to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJQoGsnC91U-",
        "outputId": "682f8f88-2b35-45f4-ef48-247c583dca2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load and Process Image\n",
        "\n",
        "# Load image\n",
        "image_path = \"/content/ex4.jpg\"\n",
        "image = Image.open(image_path)"
      ],
      "metadata": {
        "id": "Q9J0Ttfp93H5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define text prompt\n",
        "text_prompt = \"a bottle. a cup.\"  # Replace with your object description\n",
        "\n",
        "# Preprocess image and text prompt\n",
        "inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Run Grounding DINO\n",
        "with torch.no_grad():\n",
        "    outputs = detection_model(**inputs)\n",
        "\n",
        "# Post-process results\n",
        "target_sizes = [image.size[::-1]]  # (height, width)\n",
        "results = processor.post_process_grounded_object_detection(\n",
        "    outputs,\n",
        "    inputs.input_ids,\n",
        "    box_threshold=0.27,\n",
        "    text_threshold=0.35,\n",
        "    target_sizes=target_sizes\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeGF9dyN96Ib",
        "outputId": "50ffc2b2-7ab9-4dd3-cf68-d4066efaeb5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: `box_threshold` is deprecated and will be removed in version 4.51.0 for `GroundingDinoProcessor.post_process_grounded_object_detection`. Use `threshold` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract bounding boxes, labels, and scores\n",
        "boxes = results[0][\"boxes\"].cpu().numpy()\n",
        "labels = results[0][\"labels\"]\n",
        "scores = results[0][\"scores\"].cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-z9TiKK-ALr",
        "outputId": "52acd053-befc-4b9d-b692-80689e7ace6b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Segment Objects Using SAM\n",
        "\n",
        "def calculate_pixel_dimensions(mask):\n",
        "    \"\"\"Calculate the actual pixel width and height of the segmented object from the mask.\"\"\"\n",
        "    if mask.ndim == 3:  # If mask is 3D, take the first channel\n",
        "        mask = mask[0]\n",
        "\n",
        "    y_indices, x_indices = np.where(mask > 0)  # Get object pixel locations\n",
        "\n",
        "    if len(y_indices) == 0 or len(x_indices) == 0:\n",
        "        return 0, 0  # No object detected\n",
        "\n",
        "    height = np.max(y_indices) - np.min(y_indices) + 1\n",
        "    width = np.max(x_indices) - np.min(x_indices) + 1\n",
        "\n",
        "    return width, height"
      ],
      "metadata": {
        "id": "eha3iZH7-Lr4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store pixel dimensions\n",
        "object_pixel_dimensions = []"
      ],
      "metadata": {
        "id": "An4wvebC-Now"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each detected object\n",
        "for idx, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    if score > 0.25:  # Confidence threshold\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "\n",
        "        # Use the bounding box center as the input point for SAM\n",
        "        input_points = [[[(x_min + x_max) / 2, (y_min + y_max) / 2]]]\n",
        "\n",
        "        # Prepare inputs for SAM\n",
        "        inputs_sam = sam_processor(\n",
        "            image,\n",
        "            input_points=input_points,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Run SAM\n",
        "        with torch.no_grad():\n",
        "            outputs_sam = sam_model(**inputs_sam)\n",
        "\n",
        "        # Post-process SAM masks\n",
        "        masks = sam_processor.image_processor.post_process_masks(\n",
        "            outputs_sam.pred_masks.cpu(),\n",
        "            inputs_sam[\"original_sizes\"].cpu(),\n",
        "            inputs_sam[\"reshaped_input_sizes\"].cpu(),\n",
        "        )\n",
        "\n",
        "        # Extract first mask\n",
        "        mask = masks[0][0, 0].cpu().numpy()  # Ensure correct shape\n",
        "\n",
        "        # Calculate pixel width and height of the actual object\n",
        "        width, height = calculate_pixel_dimensions(mask)\n",
        "        object_pixel_dimensions.append((width, height))\n",
        "\n",
        "        # Display results\n",
        "        print(f\"Object {idx + 1} ({label}): Width = {width} px, Height = {height} px\")\n",
        "\n",
        "        # Visualize the mask\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(mask, cmap=\"gray\")\n",
        "        plt.title(f\"Segmented Object {idx + 1} - {label}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "hKUA4W83-P8T",
        "outputId": "10fc4160-6c65-4e3d-d302-b5a32e68049f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object 1 (a bottle): Width = 557 px, Height = 2128 px\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Estimate Depth Using DPT-Large (NYU Depth V2)\n",
        "inputs_depth = depth_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_depth = depth_model(**inputs_depth)\n",
        "    predicted_depth = outputs_depth.predicted_depth\n",
        "\n",
        "# Resize depth map to match original image\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ")"
      ],
      "metadata": {
        "id": "sW37xcVd-T8H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth_map = prediction.squeeze().cpu().numpy()"
      ],
      "metadata": {
        "id": "PpfGKo3V-X4X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize depth values (NYU Depth V2 is in meters)\n",
        "depth_map = depth_map / depth_map.max()"
      ],
      "metadata": {
        "id": "gqbOtDqZ-ZZw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store estimated depths\n",
        "estimated_depths = []"
      ],
      "metadata": {
        "id": "A-7XMxCH-czX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract depth for each detected object using segmentation mask\n",
        "for idx, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    if score > 0.25:\n",
        "        mask = masks[0][0].cpu().numpy()\n",
        "        if mask.ndim == 3:  # Ensure mask is 2D\n",
        "            mask = mask[0]\n",
        "\n",
        "        object_depths = depth_map[mask > 0]  # Select only the depths where the object is detected\n",
        "\n",
        "        if object_depths.size > 0:\n",
        "            median_depth = np.median(object_depths)  # Use median to reduce noise\n",
        "        else:\n",
        "            median_depth = 0  # Default to zero if no depth values found\n",
        "\n",
        "        estimated_depths.append(median_depth)\n",
        "        print(f\"Object {idx + 1} ({label}): Estimated Depth = {median_depth:.2f} meters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Upav_q-gRE",
        "outputId": "cc33c304-c03e-4861-cb70-7aa8006cece8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object 1 (a bottle): Estimated Depth = 0.81 meters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Extract Focal Length from Image Metadata\n",
        "\n",
        "def get_focal_length(image_path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        tags = exifread.process_file(f)\n",
        "\n",
        "    focal_length = tags.get(\"EXIF FocalLength\", \"N/A\")\n",
        "    if focal_length != \"N/A\":\n",
        "        focal_length = focal_length.values[0].num / focal_length.values[0].den\n",
        "    return focal_length"
      ],
      "metadata": {
        "id": "oE5wA9sw-iV-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and print focal length\n",
        "focal_length = get_focal_length(image_path)\n",
        "print(f\"📷 Focal Length: {focal_length} mm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkFQGC8y-i3h",
        "outputId": "07f78aaf-492c-47e9-d0c2-d72f613a1699"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📷 Focal Length: 4.74 mm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_focal_length_to_pixels(focal_length_mm, image_width, sensor_width=8.4):\n",
        "    return (focal_length_mm * image_width) / sensor_width"
      ],
      "metadata": {
        "id": "Dw3lTgID-nBi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "focal_length_px = convert_focal_length_to_pixels(\n",
        "    focal_length_mm=focal_length,\n",
        "    image_width=image.size[0]\n",
        ")"
      ],
      "metadata": {
        "id": "IxyAyeIY-pri"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"📷 Focal Length in Pixels: {focal_length_px:.2f} px\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfoFphvr-r2w",
        "outputId": "3438f07f-d96a-4429-ab4e-d5952885b53f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📷 Focal Length in Pixels: 1959.20 px\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate real-world object size\n",
        "def calculate_real_world_size(pixel_size, depth, focal_length_px):\n",
        "    \"\"\"\n",
        "    Convert object pixel size to real-world size using pinhole camera model.\n",
        "\n",
        "    :param pixel_size: Width or height of the object in pixels\n",
        "    :param depth: Estimated depth of the object in meters\n",
        "    :param focal_length_px: Focal length in pixels\n",
        "    :return: Real-world size in meters\n",
        "    \"\"\"\n",
        "    return (pixel_size * depth) / focal_length_px"
      ],
      "metadata": {
        "id": "zMOg2VG3-s8A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store real-world dimensions\n",
        "real_world_sizes = []"
      ],
      "metadata": {
        "id": "1lMpxmKc_KuD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate real-world sizes for each detected object\n",
        "for idx, ((pixel_width, pixel_height), depth, label) in enumerate(zip(object_pixel_dimensions, estimated_depths, labels)):\n",
        "    real_width = calculate_real_world_size(pixel_width, depth, focal_length_px)\n",
        "    real_height = calculate_real_world_size(pixel_height, depth, focal_length_px)\n",
        "\n",
        "    real_world_sizes.append((real_width, real_height))\n",
        "\n",
        "    print(f\"📏 Object {idx + 1} ({label}):\")\n",
        "    print(f\"   - Real Width: {real_width:.2f} meters\")\n",
        "    print(f\"   - Real Height: {real_height:.2f} meters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKFAji0y_Mce",
        "outputId": "1bc189e4-6ea2-4ae6-e04b-ef173d894633"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📏 Object 1 (a bottle):\n",
            "   - Real Width: 0.23 meters\n",
            "   - Real Height: 0.88 meters\n"
          ]
        }
      ]
    }
  ]
}